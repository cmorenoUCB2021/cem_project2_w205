{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poject 2: \n",
    "\n",
    "### I. Summary Business Questions\n",
    "\n",
    "1. How many assesstments are in the dataset?\n",
    "2. What's the name of your Kafka topic? How did you come up with that name?\n",
    "3. How many people took *Learning Git*?\n",
    "4. What is the least common course taken? \n",
    "5. What is the most common course taken?\n",
    "6. What are the tests with the highest percent of correct questions?\n",
    "7. What are the tests with the lowest percent of correct questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Executive Summary\n",
    "For this report, a dataset of 103 test for courses taken between November 2017 and January 2018 was evaluated.  This dataset includes 3280 observations of people taken the courses. The dataset was downloaded from the internet with the following command:\n",
    "\n",
    "```\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "```\n",
    "\n",
    "Once the JSON file was downloaded, the data was analyzed using pyspark/spark.sql which read information sent through a 'kafka' topic called 'examtaken', as the dataset includes information of the exams (assessments) associated to (103) courses.\n",
    "\n",
    "The top 5 (most common) courses in the dataset are presented in the following table:\n",
    "\n",
    "```\n",
    "+--------------------+--------+                                                 \n",
    "|           exam_name|num_exam|\n",
    "+--------------------+--------+\n",
    "|        Learning Git|     394|\n",
    "|Introduction to P...|     162|\n",
    "|Intermediate Pyth...|     158|\n",
    "|Introduction to J...|     158|\n",
    "|Learning to Progr...|     128|\n",
    "+--------------------+--------+\n",
    "```\n",
    "\n",
    "The most common course is \"Learning Git\", which was taken by 394 people. Perhaps, aligned with the current trend of professionals looking to up-skill in datascience related topics, three out of the top five courses are related to learning Python and R, which are two of the most popular programming languages for someone seeking to get into the datascience field. In addition, these top courses are mostly introductory and/or intermediate level, which may also explain their popularity, as they are apealing to a larger segment of potential customers.  In essence, these course are directed to customers seeking introductory to intermediate level of educations for Git, Python, R and Java.\n",
    "\n",
    "The least common courses (bottom 5) are presented in the following table:\n",
    "```\n",
    "+--------------------+--------+                                                 \n",
    "|           exam_name|num_exam|\n",
    "+--------------------+--------+\n",
    "|Nulls, Three-valu...|       1|\n",
    "|Native Web Apps f...|       1|\n",
    "|Learning to Visua...|       1|\n",
    "|Operating Red Hat...|       1|\n",
    "|Understanding the...|       2|\n",
    "+--------------------+--------+\n",
    "```\n",
    "These list of courses (least common) may be more on the technical side, and they do not seem to be introductory courses which may also explain the small number of participants taking them. In addition, their topics may not be as popular as introductory datascience related topics.\n",
    "\n",
    "To further analyze the data, the following csv tables were generated using 'spark.sql' queries:\n",
    "\n",
    "- test_rank.csv : includes a list of all 103 tests and the number of people who took them.\n",
    "- questions.csv : includes the sum of total questions for the test and the number of correct questions.  This will be used to identify the percentage of correct questions for each test (on average).\n",
    "- questions_avg.csv: including average percent of correct questions for the tests.\n",
    "\n",
    "The courses with the lowest percent of correct questions are:\n",
    "```\n",
    "+--------------------+--------+------------------+                              \n",
    "|           exam_name|num_exam|           percent|\n",
    "+--------------------+--------+------------------+\n",
    "|Example Exam For ...|       5|              null|\n",
    "|Client-Side Data ...|       2|               0.2|\n",
    "|Native Web Apps f...|       1|              0.25|\n",
    "|       View Updating|       4|              0.25|\n",
    "|Arduino Prototypi...|       2|0.3333333333333333|\n",
    "+--------------------+--------+------------------+\n",
    "```\n",
    "The above table present courses which should be reviewed to ensure the customer experience is adequate.  While it is important that participants demonstrate what they have learned with a test, low scores may further discourage them for recommending the courses (if test is too difficult).  In addition, low scores may also reflect a decrease in the interest of the course, or misalignment between what is been presented in the course and what is being evaluated.  Thus, these courses should be further reviewed to identify potential causes for the low scores, and to identify ways in which the customer experience is improved.\n",
    "\n",
    "The courses with the highest percent of correct questions area:\n",
    "\n",
    "```\n",
    "+--------------------+--------+------------------+                              \n",
    "|           exam_name|num_exam|           percent|\n",
    "+--------------------+--------+------------------+\n",
    "|Learning to Visua...|       1|               1.0|\n",
    "|The Closed World ...|       2|               1.0|\n",
    "|Nulls, Three-valu...|       1|               1.0|\n",
    "|Learning SQL for ...|      11|0.9772727272727273|\n",
    "|Introduction to J...|     158|0.8759493670886076|\n",
    "+--------------------+--------+------------------+\n",
    "```\n",
    "\"Introduction to Java 9\" is both in the list of the most popular courses (#3) and the list of the highest percent of correct answers (#5).  Another relatively popular course in this list is \"Learning SQL for Oracle\".\n",
    "\n",
    "**Recommendations:**  \n",
    "  \n",
    "(1) Important to continue to strengthen our offering of introductory to intermediate courses for topics related to datascience (Python and R).  These topics are in high demand, and many people are seeking to upskill on these topics.  \n",
    "(2) There are topics that have less demand, and we should re-evaluate priorities.  For example, technical (relatively advanced) courses related to Android Application Development, Client Side and/or Server related topics may be in low demand.  \n",
    "(3) While it is important that participants demonstrate what they have learned with a test, low scores may further discourage them for recommending the courses (if test is too difficult).  In addition, low scores may also reflect a decrease in the interest of the course, or misalignment between what is been presented in the course and what is being evaluated.  Thus, courses with the lowest percent of correct questions should be further reviewed to identify potential causes for the low scores, and to identify ways in which the customer experience may be improved  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Steps for Running the Project\n",
    "\n",
    "#### (1). Setup Docker Compose, Get Data and Explore Data Structure:\n",
    "\n",
    "- **Create a 'docker-compose.yml' file** including the following services: zookeeper, kafka, cloudera, spark, mids.  Please see details for yml file in **\"Structure for docker-compose.yml file\"** below.\n",
    "  \n",
    "- **Get JSON file with information**.  The name of the file is \"assessment-attempts-20180128-121051-nested.json\" - using the following command:\n",
    "\n",
    "```\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "```\n",
    "\n",
    "- **Explore the JSON file with JQ** to get an idea of the structure - the following command was used to understand the fields included in the JSON file:\n",
    "\n",
    "```\n",
    "cat assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | head -1 | jq .\n",
    "```\n",
    "\n",
    "This indicates that there are 10 fields in the main level of the JSON file: (1) 'keen_timestamp', (2) 'max_attempts', (3) 'started_at', (4) 'base_exam_id', (5) 'user_exam_id', (6) 'sequences', (7) 'keen_created_at', (8) 'certification', (9) 'keen_id', (10) 'exam_name'.  \n",
    "\n",
    "In addition, the field **\"sequences\"** has four nested levels which are: 'questions', 'attempt', 'id' and 'counts'.    \n",
    "\n",
    "Among the nested fields within sequences, **questions** amd **counts** have additional nested fields.  The field **counts** summarize key results of the test in the following fields: 'incomplete', 'submitted', 'incorrect', 'all_correct', 'correct', 'total', 'unanswered'.  These fields may be useful to understand performance for students taking these exams or courses. \n",
    "\n",
    "**Note:** An example of the data structure for JSON file is include in section **\"Top Line Structure of JSON File\"** below.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2). Run Docker Compose, Define Kafka Topic and Produce Data to Kafka Topic\n",
    "- **Once the docker-compose.yml file has been created, perform a pull to update the services and images:**\n",
    "\n",
    "```\n",
    "docker-compose pull\n",
    "```\n",
    "\n",
    "- **Run docker compose:**\n",
    "\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "- **Check logs for kafka (use CTRL-C to stop):**\n",
    "\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "```\n",
    "\n",
    "- **Create a topic for Kafka.** The name of the topic is \"examtaken\" as the dataset relates to exams taken for courses from November 2017 to January 2018.  The topic was created with the following command:  \n",
    "\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic examtaken --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "  \n",
    "- **Check for kafka topic:**\n",
    "\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --describe --topic examtaken --zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "- **Use kafkacat to produce test messages to the `examtaken` topic:**   \n",
    "\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-cmorenoUCB2021/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t examtaken\"\n",
    "```\n",
    "\n",
    "- ** Run spark to make sure it is available when running Jupyter Notebook:**\n",
    "\n",
    "```\n",
    "docker-compose exec spark ln -s /w205 w205\n",
    "```\n",
    "\n",
    "- **Run Jupyter Notebook in Google Cloud to read kafka topic and explore data:**\n",
    "```\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 7000 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```\n",
    "\n",
    "- **Get the token and include the address for your notebook instance in Google Cloud:** For example:\n",
    "```\n",
    "http://0.0.0.0:7000/?token=3b2fdecc35eaf98a3de6cbcaf30fede3349c3643fca23654\n",
    "```\n",
    "Replace 0.0.0.0 with the address associated to your Google Cloud Instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3). Use \"Pyspark\" within Jupyter Notebook to read raw data from kafka topic:\n",
    "\n",
    "- **Import libraries to work with file:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data manipulation and transformations through Spark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "'''Setting spark config to a timezone'''\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "# Reading json file format\n",
    "import json\n",
    "#Other standard python libraries\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Read from kafka (using the topic 'examtaken':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exams_raw = spark.read.format('kafka').option('kafka.bootstrap.servers','kafka:29092').option('subscribe','examtaken').option('startingOffsets','earliest').option('endingOffsets', 'latest').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------+---------+------+--------------------+-------------+\n",
      "| key|               value|    topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+---------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|examtaken|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|examtaken|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|examtaken|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|examtaken|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|examtaken|        0|     4|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+---------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check what was read\n",
    "exams_raw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cache this to cut back on warnings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exams_raw.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Create DataFrame with values converted to string:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exams_str = exams_raw.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4). Extract Key Information in a flat JSON format¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.json(exams_str.rdd.map(lambda x : x.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|  keen_created_at|             keen_id|   keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...|1516717442.735266|5a6745820eb8ab000...|1516717442.735266|         1.0|[1,[false,2,1,1,4...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...|1516717377.639827|5a674541ab6b0a000...|1516717377.639827|         1.0|[1,[false,1,2,1,4...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738973.653394|5a67999d3ed3e3000...|1516738973.653394|         1.0|[1,[false,3,0,1,4...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "+--------------------+-------------+--------------------+-----------------+--------------------+-----------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Present first three rows of what was read\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Another Alternative to read JSON nested file into a dataframe:**  \n",
    "Create a function to parser only the needed nested fields: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exam_details_from_json(row):\n",
    "    exams = json.loads(row.value)\n",
    "    exam_details = {'exam_name': exams['exam_name'],\n",
    "                     'keen_id': exams['keen_id'],\n",
    "                     'started_at': exams['started_at'],\n",
    "                     'certification': exams['certification'],\n",
    "                     'count_total': exams['sequences']['counts']['total'],\n",
    "                     'count_correct': exams['sequences']['counts']['correct'],\n",
    "                     'count_incorrect': exams['sequences']['counts']['incorrect'],\n",
    "                     'per_correct': exams['sequences']['counts']['correct'] / exams['sequences']['counts']['total'],\n",
    "                     'attempts': exams['sequences']['attempt']}\n",
    "    return Row(**exam_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Apply the function to parse information to string, and save them into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = exams_str.rdd.map(extract_exam_details_from_json).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+---------------+-----------+--------------------+--------------------+-----------+--------------------+\n",
      "|attempts|certification|count_correct|count_incorrect|count_total|           exam_name|             keen_id|per_correct|          started_at|\n",
      "+--------+-------------+-------------+---------------+-----------+--------------------+--------------------+-----------+--------------------+\n",
      "|       1|        false|            2|              1|          4|Normal Forms and ...|5a6745820eb8ab000...|        0.5|2018-01-23T14:23:...|\n",
      "|       1|        false|            1|              1|          4|Normal Forms and ...|5a674541ab6b0a000...|       0.25|2018-01-23T14:21:...|\n",
      "|       1|        false|            3|              1|          4|The Principles of...|5a67999d3ed3e3000...|       0.75|2018-01-23T20:22:...|\n",
      "+--------+-------------+-------------+---------------+-----------+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Present first three rows of what was read/converted:\n",
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5). Use SparkSQL to explore data and answer business questions:\n",
    "\n",
    "- **Create a Spark \"TempTable\" (or \"View\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.registerTempTable('examview')\n",
    "df1.registerTempTable('examview1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Use SparkSQL to answer business questions:**\n",
    "  \n",
    "  \n",
    "*1. How many assesstments are in the dataset?:* There are 3280 entries in the dataset, where each entry represent a test/course taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(exam_name)|\n",
      "+----------------+\n",
      "|            3280|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(exam_name) from examview\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. How many tests are there in the dataset?* There are 103 different courses in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT exam_name)|\n",
      "+-------------------------+\n",
      "|                      103|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct exam_name) from examview\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. How many people took *Learning Git*?*  394 people took the course \"Learning Git\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(exam_name)|\n",
      "+----------------+\n",
      "|             394|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(exam_name) from examview WHERE  exam_name = 'Learning Git' \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*4. What is the least common course taken?* The 5 least common courses are shown in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           exam_name|num_exam|\n",
      "+--------------------+--------+\n",
      "|Nulls, Three-valu...|       1|\n",
      "|Native Web Apps f...|       1|\n",
      "|Learning to Visua...|       1|\n",
      "|Operating Red Hat...|       1|\n",
      "|Understanding the...|       2|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(keen_id) as num_exam FROM examview GROUP BY exam_name ORDER BY num_exam limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*5. What is the most common course taken?* The top five courses taken are presented in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           exam_name|num_exam|\n",
      "+--------------------+--------+\n",
      "|        Learning Git|     394|\n",
      "|Introduction to P...|     162|\n",
      "|Introduction to J...|     158|\n",
      "|Intermediate Pyth...|     158|\n",
      "|Learning to Progr...|     128|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_numexam = spark.sql('select exam_name, count(keen_id) as num_exam FROM examview GROUP BY exam_name ORDER BY num_exam DESC')\n",
    "df_numexam.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*6. Query to extract the count information for each test.  Dataframe to be saved in a local CSV file to be presented in NoteBook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+---------+\n",
      "|           exam_name|num_exam|       avg_correct|avg_total|\n",
      "+--------------------+--------+------------------+---------+\n",
      "|        Learning Git|     394|3.3807106598984773|      5.0|\n",
      "|Introduction to P...|     162|2.8333333333333335|      5.0|\n",
      "|Introduction to J...|     158| 4.379746835443038|      5.0|\n",
      "|Intermediate Pyth...|     158| 2.050632911392405|      4.0|\n",
      "|Learning to Progr...|     128|            3.8125|      7.0|\n",
      "+--------------------+--------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions_avg = spark.sql(\"select exam_name, count(keen_id) as num_exam, avg(sequences.counts.correct) as avg_correct, avg(sequences.counts.total) as avg_total from examview GROUP BY exam_name ORDER BY num_exam DESC\")\n",
    "questions_avg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dataframe can be saved as a CSV file, and then read it again (to present tables and visualizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>exam_name</th>\n",
       "      <th>num_exam</th>\n",
       "      <th>avg_correct</th>\n",
       "      <th>avg_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Learning Git</td>\n",
       "      <td>394</td>\n",
       "      <td>3.380711</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction to Python</td>\n",
       "      <td>162</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Introduction to Java 8</td>\n",
       "      <td>158</td>\n",
       "      <td>4.379747</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Intermediate Python Programming</td>\n",
       "      <td>158</td>\n",
       "      <td>2.050633</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Learning to Program with R</td>\n",
       "      <td>128</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        exam_name  num_exam  avg_correct  \\\n",
       "0           0                     Learning Git       394     3.380711   \n",
       "1           1           Introduction to Python       162     2.833333   \n",
       "2           2           Introduction to Java 8       158     4.379747   \n",
       "3           3  Intermediate Python Programming       158     2.050633   \n",
       "4           4       Learning to Program with R       128     3.812500   \n",
       "\n",
       "   avg_total  \n",
       "0        5.0  \n",
       "1        5.0  \n",
       "2        5.0  \n",
       "3        4.0  \n",
       "4        7.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_avg.toPandas().to_csv(\"///w205/cem_project2_w205/questions_avg.csv\")\n",
    "df = pd.read_csv(\"questions_avg.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query including AVERAGE for total and correct questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Query including AVERAGE for total and correct questions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_name</th>\n",
       "      <th>num_exam</th>\n",
       "      <th>avg_correct</th>\n",
       "      <th>avg_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learning Git</td>\n",
       "      <td>394</td>\n",
       "      <td>3.380711</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Introduction to Python</td>\n",
       "      <td>162</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to Java 8</td>\n",
       "      <td>158</td>\n",
       "      <td>4.379747</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermediate Python Programming</td>\n",
       "      <td>158</td>\n",
       "      <td>2.050633</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning to Program with R</td>\n",
       "      <td>128</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         exam_name  num_exam  avg_correct  avg_total\n",
       "0                     Learning Git       394     3.380711        5.0\n",
       "1           Introduction to Python       162     2.833333        5.0\n",
       "2           Introduction to Java 8       158     4.379747        5.0\n",
       "3  Intermediate Python Programming       158     2.050633        4.0\n",
       "4       Learning to Program with R       128     3.812500        7.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_avg = spark.sql(\"select exam_name, count(keen_id) as num_exam, avg(sequences.counts.correct) as avg_correct, avg(sequences.counts.total) as avg_total from examview GROUP BY exam_name ORDER BY num_exam DESC\")\n",
    "df_question_avg = questions_avg.toPandas()\n",
    "df_question_avg.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What classes have the highest and lowest percent of correct questions?**\n",
    "\n",
    "#Create a View with Average Total Questions and Average Correct Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_avg.registerTempTable('exam_avg_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7.1 What classes have the highest percent of correct questions?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+\n",
      "|           exam_name|num_exam|           percent|\n",
      "+--------------------+--------+------------------+\n",
      "|Learning to Visua...|       1|               1.0|\n",
      "|The Closed World ...|       2|               1.0|\n",
      "|Nulls, Three-valu...|       1|               1.0|\n",
      "|Learning SQL for ...|      11|0.9772727272727273|\n",
      "|Introduction to J...|     158|0.8759493670886076|\n",
      "+--------------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, num_exam, avg_correct/avg_total as percent  from exam_avg_view ORDER BY percent DESC limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 7.2 What classes have the lowest percent of correct questions?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+\n",
      "|           exam_name|num_exam|           percent|\n",
      "+--------------------+--------+------------------+\n",
      "|Example Exam For ...|       5|              null|\n",
      "|Client-Side Data ...|       2|               0.2|\n",
      "|Native Web Apps f...|       1|              0.25|\n",
      "|       View Updating|       4|              0.25|\n",
      "|Arduino Prototypi...|       2|0.3333333333333333|\n",
      "+--------------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, num_exam, avg_correct/avg_total as percent  from exam_avg_view ORDER BY percent limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Structure for 'docker-compose.yml' file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---\n",
    "version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/cdh-minimal:latest\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"50070\" # nn http\n",
    "      - \"8888\" # hue\n",
    "    #ports:\n",
    "    #- \"8888:8888\"\n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.5\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "    expose:\n",
    "      - \"7000\"\n",
    "    ports:\n",
    "      - \"7000:7000\"\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Top Line Structure of JSON File:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  \"keen_timestamp\": \"1516717442.735266\",\n",
    "  \"max_attempts\": \"1.0\",\n",
    "  \"started_at\": \"2018-01-23T14:23:19.082Z\",\n",
    "  \"base_exam_id\": \"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\n",
    "  \"user_exam_id\": \"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\n",
    "  \"sequences\": {\n",
    "    \"questions\": [\n",
    "      {\n",
    "        \"user_incomplete\": true,\n",
    "        \"user_correct\": false,\n",
    "        \"options\": [\n",
    "          {\n",
    "            \"checked\": true,\n",
    "            \"at\": \"2018-01-23T14:23:24.670Z\",\n",
    "            \"id\": \"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\n",
    "            \"submitted\": 1,\n",
    "            \"correct\": true\n",
    "          },\n",
    "          {\n",
    "            \"checked\": true,\n",
    "            \"at\": \"2018-01-23T14:23:25.914Z\",\n",
    "            \"id\": \"f2528210-35c3-4320-acf3-9056567ea19f\",\n",
    "            \"submitted\": 1,\n",
    "            \"correct\": true\n",
    "          },\n",
    "          {\n",
    "            \"checked\": false,\n",
    "            \"correct\": true,\n",
    "            \"id\": \"d1bf026f-554f-4543-bdd2-54dcf105b826\"\n",
    "          }\n",
    "        ],\n",
    "        \"user_submitted\": true,\n",
    "        \"id\": \"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\n",
    "        \"user_result\": \"missed_some\"\n",
    "      },\n",
    "      {\n",
    "        \"user_incomplete\": false,\n",
    "        \"user_correct\": false,\n",
    "        \"options\": [\n",
    "          {\n",
    "            \"checked\": true,\n",
    "            \"at\": \"2018-01-23T14:23:30.116Z\",\n",
    "            \"id\": \"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\n",
    "            \"submitted\": 1\n",
    "          },\n",
    "          {\n",
    "            \"checked\": false,\n",
    "            \"correct\": true,\n",
    "            \"id\": \"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"\n",
    "          },\n",
    "          {\n",
    "            \"checked\": true,\n",
    "            \"at\": \"2018-01-23T14:23:41.791Z\",\n",
    "            \"id\": \"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\n",
    "            \"submitted\": 1,\n",
    "            \"correct\": true\n",
    "          }\n",
    "        ],\n",
    "        \"user_submitted\": true,\n",
    "        \"id\": \"bbed4358-999d-4462-9596-bad5173a6ecb\",\n",
    "        \"user_result\": \"incorrect\"\n",
    "      },\n",
    "      {\n",
    "        \"user_incomplete\": false,\n",
    "        \"user_correct\": true,\n",
    "        \"options\": [\n",
    "          {\n",
    "            \"checked\": false,\n",
    "            \"at\": \"2018-01-23T14:23:52.510Z\",\n",
    "            \"id\": \"a9333679-de9d-41ff-bb3d-b239d6b95732\"\n",
    "          },\n",
    "          {\n",
    "            \"checked\": false,\n",
    "            \"id\": \"85795acc-b4b1-4510-bd6e-41648a3553c9\"\n",
    "          },\n",
    "          {\n",
    "            \"checked\": true,\n",
    "            \"at\": \"2018-01-23T14:23:54.223Z\",\n",
    "            \"id\": \"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\n",
    "            \"submitted\": 1,\n",
    "            \"correct\": true\n",
    "          },\n",
    "          {\n",
    "            \"checked\": true,\n",
    "            \"at\": \"2018-01-23T14:23:53.862Z\",\n",
    "            \"id\": \"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\n",
    "            \"submitted\": 1,\n",
    "            \"correct\": true\n",
    "          }\n",
    "        ],\n",
    "        \"user_submitted\": true,\n",
    "        \"id\": \"e6ad8644-96b1-4617-b37b-a263dded202c\",\n",
    "        \"user_result\": \"correct\"\n",
    "      },\n",
    "      {\n",
    "        \"user_incomplete\": false,\n",
    "        \"user_correct\": true,\n",
    "        \"options\": [\n",
    "          {\n",
    "            \"checked\": false,\n",
    "            \"id\": \"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"\n",
    "          },\n",
    "          {\n",
    "            \"checked\": false,\n",
    "            \"id\": \"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"\n",
    "          },\n",
    "          {\n",
    "            \"checked\": false,\n",
    "            \"id\": \"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"\n",
    "          },\n",
    "          {\n",
    "            \"checked\": true,\n",
    "            \"at\": \"2018-01-23T14:24:00.807Z\",\n",
    "            \"id\": \"7f13df9c-fcbe-4424-914f-2206f106765c\",\n",
    "            \"submitted\": 1,\n",
    "            \"correct\": true\n",
    "          }\n",
    "        ],\n",
    "        \"user_submitted\": true,\n",
    "        \"id\": \"95194331-ac43-454e-83de-ea8913067055\",\n",
    "        \"user_result\": \"correct\"\n",
    "      }\n",
    "    ],\n",
    "    \"attempt\": 1,\n",
    "    \"id\": \"5b28a462-7a3b-42e0-b508-09f3906d1703\",\n",
    "    \"counts\": {\n",
    "      \"incomplete\": 1,\n",
    "      \"submitted\": 4,\n",
    "      \"incorrect\": 1,\n",
    "      \"all_correct\": false,\n",
    "      \"correct\": 2,\n",
    "      \"total\": 4,\n",
    "      \"unanswered\": 0\n",
    "    }\n",
    "  },\n",
    "  \"keen_created_at\": \"1516717442.735266\",\n",
    "  \"certification\": \"false\",\n",
    "  \"keen_id\": \"5a6745820eb8ab00016be1f1\",\n",
    "  \"exam_name\": \"Normal Forms and All That Jazz Master Class\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. File and Other Information about this Project:\n",
    "\n",
    "(a) The file used for this project was downloaded using the following command:\n",
    "\n",
    "```\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "```\n",
    "(b) This file is in JSON format with 10 fields in at the main level: (1) 'keen_timestamp', (2) 'max_attempts', (3) 'started_at', (4) 'base_exam_id', (5) 'user_exam_id', (6) 'sequences', (7) 'keen_created_at', (8) 'certification', (9) 'keen_id', (10) 'exam_name'.  \n",
    "\n",
    "In addition, the field **\"sequences\"** has four nested levels which are: 'questions', 'attempt', 'id' and 'counts'.    \n",
    "\n",
    "Among the nested fields within sequences, **questions** amd **counts** have additional nested fields.  The field **counts** summarize key results of the test in the following fields: 'incomplete', 'submitted', 'incorrect', 'all_correct', 'correct', 'total', 'unanswered'.  These fields may be useful to understand performance for students taking these exams or courses.\n",
    "\n",
    "(c) This project was implemented using a 'docker-compose' including the following services: zookeeper, kafka, spark, and MIDS. The docker-compose.yml file structure is included in section V.\n",
    "\n",
    "(d) A kafka topic (examtaken) was created and used to send the information to be analyzed using pyspark/spark.sql. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
